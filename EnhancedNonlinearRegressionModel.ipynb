{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e518c1c4-ce76-483f-92f0-70b0f25dbe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-choi\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os \n",
    "\n",
    "# Define the path to your tar.gz file\n",
    "os.chdir('/home/jupyter-choi/')\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e395240a-ed80-491d-9ea9-e689eff01541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk  \n",
    "import matplotlib.pyplot as plt\n",
    "def find_pqr(signal, sr):\n",
    "    try:\n",
    "        signals = signal.reshape(-1,)\n",
    "        signals = nk.ecg_clean(signals, sr, method='neurokit') #디노이징\n",
    "        _, rpeaks = nk.ecg_peaks(signals, sampling_rate = sr)\n",
    "        _, waves_peak = nk.ecg_delineate(signals, rpeaks, sampling_rate = sr, method='peak')\n",
    "\n",
    "        r_peaks = np.array(rpeaks['ECG_R_Peaks'])\n",
    "        r_peaks = r_peaks[~np.isnan(r_peaks)].astype('int')\n",
    "\n",
    "        p_peaks = np.array(waves_peak['ECG_P_Peaks'])\n",
    "        p_peaks = p_peaks[~np.isnan(p_peaks)].astype('int')\n",
    "\n",
    "        q_peaks = np.array(waves_peak['ECG_Q_Peaks'])\n",
    "        q_peaks = q_peaks[~np.isnan(q_peaks)].astype('int')\n",
    "\n",
    "        s_peaks = np.array(waves_peak['ECG_S_Peaks'])\n",
    "        s_peaks = s_peaks[~np.isnan(s_peaks)].astype('int')\n",
    "\n",
    "        t_peaks = np.array(waves_peak['ECG_T_Peaks'])\n",
    "        t_peaks = t_peaks[~np.isnan(t_peaks)].astype('int')\n",
    "\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(signals)\n",
    "        plt.plot(p_peaks, signals[p_peaks], \"o\", markersize = 6, label = 'P_peaks')\n",
    "        plt.plot(q_peaks, signals[q_peaks], \"o\", markersize = 6, label = 'Q_peaks')\n",
    "        plt.plot(r_peaks, signals[r_peaks], \"o\", markersize = 6, label = 'R_peaks')\n",
    "        plt.plot(s_peaks, signals[s_peaks], \"o\", markersize = 6, label = 'S_peaks')\n",
    "        plt.plot(t_peaks, signals[t_peaks], \"o\", markersize = 6, label = 'T_peaks')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "        # 오류가 발생한 경우 NaN 배열을 반환하여 시각화 부분에서 건너뜁니다.\n",
    "        return np.full_like(signal, np.nan)\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241a3a6f-821c-4d79-bd4e-ecac61bfbe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1phymlyh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>loss</td><td>█▂▁▁▁▂▃▃▂▁</td></tr><tr><td>test_mse</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>test_rmse</td><td>▁</td></tr><tr><td>train_mse</td><td>▁</td></tr><tr><td>train_r2</td><td>▁</td></tr><tr><td>train_rmse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>841.44612</td></tr><tr><td>test_mse</td><td>337525.34375</td></tr><tr><td>test_r2</td><td>-206.9627</td></tr><tr><td>test_rmse</td><td>580.96931</td></tr><tr><td>train_mse</td><td>827.43188</td></tr><tr><td>train_r2</td><td>0.12265</td></tr><tr><td>train_rmse</td><td>28.76512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-dust-43</strong> at: <a href='https://wandb.ai/chwo67/Linear_AdamW/runs/1phymlyh' target=\"_blank\">https://wandb.ai/chwo67/Linear_AdamW/runs/1phymlyh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./diffusion/code/wandb/run-20231124_164316-1phymlyh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1phymlyh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-melee/diffusion/code/wandb/run-20231124_164506-6krfncx6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chwo67/Linear_AdamW/runs/6krfncx6' target=\"_blank\">peachy-pond-44</a></strong> to <a href='https://wandb.ai/chwo67/Linear_AdamW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chwo67/Linear_AdamW' target=\"_blank\">https://wandb.ai/chwo67/Linear_AdamW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chwo67/Linear_AdamW/runs/6krfncx6' target=\"_blank\">https://wandb.ai/chwo67/Linear_AdamW/runs/6krfncx6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "Epoch 1, Loss 901.4700815429687\n",
      "Epoch 2, Loss 860.7278571777343\n",
      "Epoch 3, Loss 845.5960678710937\n",
      "Epoch 4, Loss 849.1218818359375\n",
      "Epoch 5, Loss 841.1818415527343\n",
      "Epoch 6, Loss 840.0620512695313\n",
      "Epoch 7, Loss 839.2981042480469\n",
      "Epoch 8, Loss 837.8954382324218\n",
      "Epoch 9, Loss 838.5261459960938\n",
      "Epoch 10, Loss 830.2317451171875\n",
      "Epoch 10 Evaluation - Train MSE: 824.5441284179688, Train RMSE: 28.714876430484054, Train R2: 0.12705934024284674\n",
      "Epoch 10 Evaluation - Test MSE: 325119.4375, Test RMSE: 570.1924565442795, Test R2: -204.56740820030683\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2000 into shape (11,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 225\u001b[0m\n\u001b[1;32m    222\u001b[0m             test_predictions_samples \u001b[38;5;241m=\u001b[39m test_predictions_reshaped[sample_indices]\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;66;03m# Plot original vs predicted\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m             \u001b[43mplot_original_vs_predicted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_predictions_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_visual_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    231\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mplot_original_vs_predicted\u001b[0;34m(original, predicted, num_samples, num_leads)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(num_samples, \u001b[38;5;28mlen\u001b[39m(original))):\n\u001b[1;32m     29\u001b[0m     original_lead \u001b[38;5;241m=\u001b[39m original[i]\n\u001b[0;32m---> 30\u001b[0m     predicted_lead \u001b[38;5;241m=\u001b[39m \u001b[43mpredicted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_leads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Plotting the original lead\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(num_samples, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2000 into shape (11,500)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from adamp import AdamP\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wandb.init(project=\"Linear_AdamW\")\n",
    "\n",
    "# CUDA 사용 가능 여부 확인 및 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 시각화를 위한 함수\n",
    "def plot_original_vs_predicted(original, predicted, num_samples=5, num_leads=11):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    lead_length = 500  # Correct lead length based on your data\n",
    "\n",
    "    for i in range(min(num_samples, len(original))):\n",
    "        original_lead = original[i]\n",
    "        predicted_lead = predicted[i].reshape(num_leads, lead_length)\n",
    "\n",
    "\n",
    "        # Plotting the original lead\n",
    "        plt.subplot(num_samples, 2, 2*i + 1)\n",
    "        plt.plot(original_lead, label=\"Original Lead\")\n",
    "        plt.title(f\"Original Lead Sample {i+1}\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Plotting a few selected predicted leads for clarity\n",
    "        plt.subplot(num_samples, 2, 2*i + 2)\n",
    "        for j in range(min(3, num_leads)):  # Plotting first 3 leads as an example\n",
    "            plt.plot(predicted_lead[j], label=f\"Predicted Lead {j+1}\")\n",
    "        plt.title(f\"Predicted Leads Sample {i+1}\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def flatten_data(data):\n",
    "    return data.reshape(data.shape[0], -1)\n",
    "\n",
    "# MSE와 RMSE 계산 함수\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    return mse, rmse\n",
    "\n",
    "# R-squared 계산 함수\n",
    "def calculate_r_squared(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def load_npy_data(folder_path):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    processed_files = set()\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith('wave') and file_name.endswith('.npy') and file_name not in processed_files:\n",
    "            data = np.load(os.path.join(folder_path, file_name))\n",
    "            processed_files.add(file_name)\n",
    "            if data.shape[0] == 13:\n",
    "                data = data[:12, :]  # 13번째 행 제거\n",
    "                \n",
    "            if data.shape[0] >= 12:\n",
    "                X_feature = data[1, :]  # Assuming 1st lead as input\n",
    "                y_features = data[:4, :]  # Remaining leads as output\n",
    "                X_list.append(X_feature)\n",
    "                y_list.append(y_features.flatten())  # Flatten the 11 leads\n",
    "            else:\n",
    "                print(f\"Invalid file format: {file_name}\")\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n",
    "\n",
    "# 회귀 모델 정의\n",
    "class EnhancedNonlinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, lead_length):\n",
    "        super(EnhancedNonlinearRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)\n",
    "        self.fc2 = nn.Linear(20, 40)\n",
    "        self.fc3 = nn.Linear(40, 20)\n",
    "        self.fc4 = nn.Linear(20, 10)\n",
    "        self.fc5 = nn.Linear(10, output_size)  # output_size를 목표 데이터의 크기에 맞게 설정\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# 데이터 로드\n",
    "folder_path = \"diffusion/data_No20000/data_0\"\n",
    "X, y = load_npy_data(folder_path)\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Flatten the target data if necessary\n",
    "y_train_flat = flatten_data(y_train_tensor.numpy())\n",
    "y_test_flat = flatten_data(y_test_tensor.numpy())\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, torch.tensor(y_train_flat, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test_flat, dtype=torch.float32))\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "input_size = 5000  # Lead II의 데이터 길이\n",
    "output_size = 5000 *4\n",
    "lead_length = 500\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# 모델 초기화 시 input_size를 올바르게 설정\n",
    "model = EnhancedNonlinearRegressionModel(input_size=X_train_tensor.shape[1], lead_length=lead_length)\n",
    "\n",
    "\n",
    "# Check available GPUs and adjust device_ids if necessary\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# Ensure the model is on the default device before applying DataParallel\n",
    "model.to('cuda:0')\n",
    "\n",
    "# Apply DataParallel\n",
    "if torch.cuda.is_available():\n",
    "    model = nn.DataParallel(model, device_ids=[0,1,2,3, 4, 5,6,7])\n",
    "\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# optimizer = AdamP(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay=1e-2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay=1e-2)\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "# Now your training loop can use train_loader\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss {avg_loss}')\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n",
    "\n",
    "    # 평가 및 wandb에 기록\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 훈련 세트 평가\n",
    "            train_predictions = model(X_train_tensor)\n",
    "            train_mse, train_rmse = calculate_performance_metrics(y_train_flat, flatten_data(train_predictions.cpu().numpy()))\n",
    "\n",
    "            # 테스트 세트 평가\n",
    "            test_predictions = model(X_test_tensor)\n",
    "            test_mse, test_rmse = calculate_performance_metrics(y_test_flat, flatten_data(test_predictions.cpu().numpy()))\n",
    "\n",
    "            # R-squared 계산\n",
    "            train_r2 = calculate_r_squared(y_train_flat, flatten_data(train_predictions.cpu().numpy()))\n",
    "            test_r2 = calculate_r_squared(y_test_flat, flatten_data(test_predictions.cpu().numpy()))\n",
    "\n",
    "            # wandb에 기록\n",
    "            wandb.log({\"epoch\": epoch, \"train_mse\": train_mse, \"train_rmse\": train_rmse, \"train_r2\": train_r2, \n",
    "                       \"test_mse\": test_mse, \"test_rmse\": test_rmse, \"test_r2\": test_r2})\n",
    "\n",
    "            print(f'Epoch {epoch+1} Evaluation - Train MSE: {train_mse}, Train RMSE: {train_rmse}, Train R2: {train_r2}')\n",
    "            print(f'Epoch {epoch+1} Evaluation - Test MSE: {test_mse}, Test RMSE: {test_rmse}, Test R2: {test_r2}')\n",
    "            # After model evaluation\n",
    "            test_predictions = model(X_test_tensor)\n",
    "            test_predictions_flat = flatten_data(test_predictions.cpu().numpy())\n",
    "\n",
    "            # Reshape the predictions to have 11 leads\n",
    "            test_predictions_reshaped = test_predictions_flat.reshape(-1, 4, lead_length)\n",
    "            num_visual_samples = 5\n",
    "            sample_indices = np.random.choice(len(X_test), num_visual_samples, replace=False)\n",
    "            X_test_samples = X_test[sample_indices]\n",
    "            test_predictions_samples = test_predictions_reshaped[sample_indices]\n",
    "\n",
    "            # Plot original vs predicted\n",
    "            plot_original_vs_predicted(X_test_samples, test_predictions_samples, num_samples=num_visual_samples)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predictions = model(X_train.unsqueeze(-1)).numpy()\n",
    "    test_predictions = model(X_test.unsqueeze(-1)).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Plot original vs predicted\n",
    "plot_original_vs_predicted(X_test.numpy(), test_predictions_reshaped, num_samples=5)\n",
    "\n",
    "# 최종 평가 결과 출력\n",
    "train_predictions = model(X_train_tensor)\n",
    "train_predictions_flat = flatten_data(train_predictions.cpu().numpy())\n",
    "train_mse, train_rmse = calculate_performance_metrics(y_train_flat, train_predictions_flat)\n",
    "print(f\"Training Set - MSE: {train_mse}, RMSE: {train_rmse}\")\n",
    "\n",
    "test_predictions = model(X_test_tensor)\n",
    "test_predictions_flat = flatten_data(test_predictions.cpu().numpy())\n",
    "test_mse, test_rmse = calculate_performance_metrics(y_test_flat, test_predictions_flat)\n",
    "print(f\"Test Set - MSE: {test_mse}, RMSE: {test_rmse}\")\n",
    "\n",
    "train_r2 = calculate_r_squared(y_train_flat, train_predictions_flat)\n",
    "print(f'Training Set - R-squared: {train_r2}')\n",
    "\n",
    "test_r2 = calculate_r_squared(y_test_flat, test_predictions_flat)\n",
    "print(f'Test Set - R-squared: {test_r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20_py310",
   "language": "python",
   "name": "pt20_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
